{"cells":[{"cell_type":"markdown","metadata":{"id":"lqt_yzRy16Wj"},"source":["## Task\n","\n","In this compulsory task you will clean the country column and parse the date column in the **store_income_data_task.csv** file."]},{"cell_type":"code","execution_count":150,"metadata":{"id":"vBP3WN2O16Wp"},"outputs":[],"source":["# Load up store_income_data.csv\n","import pandas as pd\n","import numpy as np\n","import fuzzywuzzy\n","from fuzzywuzzy import process\n","import chardet\n","import datetime\n"]},{"cell_type":"code","execution_count":151,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>store_name</th>\n","      <th>store_email</th>\n","      <th>department</th>\n","      <th>income</th>\n","      <th>date_measured</th>\n","      <th>country</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>Cullen/Frost Bankers, Inc.</td>\n","      <td>NaN</td>\n","      <td>Clothing</td>\n","      <td>$54438554.24</td>\n","      <td>4-2-2006</td>\n","      <td>United States/</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>Nordson Corporation</td>\n","      <td>NaN</td>\n","      <td>Tools</td>\n","      <td>$41744177.01</td>\n","      <td>4-1-2006</td>\n","      <td>Britain</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>Stag Industrial, Inc.</td>\n","      <td>NaN</td>\n","      <td>Beauty</td>\n","      <td>$36152340.34</td>\n","      <td>12-9-2003</td>\n","      <td>United States</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>FIRST REPUBLIC BANK</td>\n","      <td>ecanadine3@fc2.com</td>\n","      <td>Automotive</td>\n","      <td>$8928350.04</td>\n","      <td>8-5-2006</td>\n","      <td>Britain/</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>Mercantile Bank Corporation</td>\n","      <td>NaN</td>\n","      <td>Baby</td>\n","      <td>$33552742.32</td>\n","      <td>21-1-1973</td>\n","      <td>United Kingdom</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id                   store_name         store_email  department  \\\n","0   1   Cullen/Frost Bankers, Inc.                 NaN    Clothing   \n","1   2          Nordson Corporation                 NaN       Tools   \n","2   3        Stag Industrial, Inc.                 NaN      Beauty   \n","3   4          FIRST REPUBLIC BANK  ecanadine3@fc2.com  Automotive   \n","4   5  Mercantile Bank Corporation                 NaN        Baby   \n","\n","         income date_measured          country  \n","0  $54438554.24      4-2-2006   United States/  \n","1  $41744177.01      4-1-2006          Britain  \n","2  $36152340.34     12-9-2003    United States  \n","3   $8928350.04      8-5-2006         Britain/  \n","4  $33552742.32     21-1-1973   United Kingdom  "]},"execution_count":151,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.read_csv(\"store_income_data_task.csv\")\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"ItqLwumA16Wr"},"source":["1. Take a look at all the unique values in the \"country\" column. Then, convert the column to lowercase and remove any trailing white spaces."]},{"cell_type":"code","execution_count":152,"metadata":{"id":"sLkzt4Hr16Wr"},"outputs":[{"name":"stdout","output_type":"stream","text":["['United States/' 'Britain' ' United States' 'Britain/' ' United Kingdom'\n"," 'U.K.' 'SA ' 'U.K/' 'America' 'United Kingdom' nan 'united states'\n"," ' S.A.' 'England ' 'UK' 'S.A./' 'ENGLAND' 'BRITAIN' 'U.K' 'U.K '\n"," 'America/' 'SA.' 'S.A. ' 'u.k' 'uk' ' ' 'UK.' 'England/' 'england'\n"," ' Britain' 'united states of america' 'UK/' 'SA/' 'SA' 'England.'\n"," 'UNITED KINGDOM' 'America.' 'S.A..' 's.a.' ' U.K'\n"," ' United States of America' 'Britain ' 'England' ' SA'\n"," 'United States of America.' 'United States of America/' 'United States.'\n"," 's. africasouth africa' ' England' 'United Kingdom '\n"," 'United States of America ' ' UK' 'united kingdom' 'AMERICA' 'America '\n"," 'UNITED STATES OF AMERICA' ' S. AfricaSouth Africa' 'america'\n"," 'S. AFRICASOUTH AFRICA' 'Britain.' '/' 'United Kingdom.' 'United States'\n"," ' America' 'UNITED STATES' 'sa' 'United States of America' 'UK '\n"," 'United States ' 'S. AfricaSouth Africa/' 'S.A.' 'United Kingdom/'\n"," 'S. AfricaSouth Africa ' 'S. AfricaSouth Africa.' 'S. AfricaSouth Africa'\n"," '.' 'britain']\n","76\n"]}],"source":["countries = df[\"country\"].unique()\n","number_of_countries = df[\"country\"].nunique()\n","print(countries)\n","print(number_of_countries)\n"]},{"cell_type":"code","execution_count":153,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['united states/' 'britain' 'united states' 'britain/' 'united kingdom'\n"," 'u.k.' 'sa' 'u.k/' 'america' nan 's.a.' 'england' 'uk' 's.a./' 'u.k'\n"," 'america/' 'sa.' '' 'uk.' 'england/' 'united states of america' 'uk/'\n"," 'sa/' 'england.' 'america.' 's.a..' 'united states of america.'\n"," 'united states of america/' 'united states.' 's. africasouth africa'\n"," 'britain.' '/' 'united kingdom.' 's. africasouth africa/'\n"," 'united kingdom/' 's. africasouth africa.' '.']\n","36\n"]}],"source":["df[\"country\"] = df[\"country\"].str.lower()\n","df[\"country\"] = df[\"country\"].str.strip()\n","countries = df[\"country\"].unique()\n","number_of_countries = df[\"country\"].nunique()\n","print(countries)\n","print(number_of_countries)\n"]},{"cell_type":"code","execution_count":154,"metadata":{},"outputs":[{"data":{"text/plain":["[('uk', 100),\n"," ('uk.', 100),\n"," ('uk/', 100),\n"," ('u.k.', 40),\n"," ('u.k/', 40),\n"," ('u.k', 40),\n"," ('united states/', 13),\n"," ('united states', 13),\n"," ('united states.', 13),\n"," ('united kingdom', 12)]"]},"execution_count":154,"metadata":{},"output_type":"execute_result"}],"source":["matches = fuzzywuzzy.process.extract(\"uk\", countries, limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n","matches"]},{"cell_type":"markdown","metadata":{"id":"P6dcDc4P16Ws"},"source":["2. Note that there should only be three separate countries. Eliminate all variations, so that 'South Africa', 'United Kingdom' and 'United States' are the only three countries."]},{"cell_type":"code","execution_count":155,"metadata":{"id":"qeV3CxMR16Ws"},"outputs":[],"source":["# Function to replace rows in the provided column of the provided DataFrame\n","# that match the provided string above the provided ratio with the provided string\n","def replace_matches_in_column(df, column, string_to_match, min_ratio = 90):\n","    # get a list of unique strings\n","    strings = df[column].unique()\n","    \n","    # Get the top 10 closest matches to our input string\n","    matches = fuzzywuzzy.process.extract(string_to_match, strings, \n","                                         limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n","\n","    # Only get matches with a ratio > 90\n","    close_matches = [matches[0] for matches in matches if matches[1] >= min_ratio]\n","\n","    # Get the rows of all the close matches in our dataframe\n","    rows_with_matches = df[column].isin(close_matches)\n","\n","    # Replace all rows with close matches with the input matches \n","    df.loc[rows_with_matches, column] = string_to_match\n","    \n","    # Let us know when the function is done\n","    print(\"All done!\")"]},{"cell_type":"code","execution_count":156,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["All done!\n","All done!\n","All done!\n"]}],"source":["replace_matches_in_column(df=df,column=\"country\",string_to_match = \"united kingdom\")\n","replace_matches_in_column(df=df,column=\"country\",string_to_match = \"united states\")\n","replace_matches_in_column(df=df,column=\"country\",string_to_match = \"south africa\")"]},{"cell_type":"code","execution_count":157,"metadata":{"id":"gMJbN84P16Wt"},"outputs":[{"name":"stdout","output_type":"stream","text":["['united states' 'britain' 'britain/' 'united kingdom' 'u.k.' 'sa' 'u.k/'\n"," 'america' nan 's.a.' 'england' 'uk' 's.a./' 'u.k' 'america/' 'sa.' ''\n"," 'uk.' 'england/' 'united states of america' 'uk/' 'sa/' 'england.'\n"," 'america.' 's.a..' 'united states of america.'\n"," 'united states of america/' 's. africasouth africa' 'britain.' '/'\n"," 's. africasouth africa/' 's. africasouth africa.' '.']\n","32\n"]}],"source":["countries = df[\"country\"].unique()\n","no_of_countries = df[\"country\"].nunique()\n","print(countries)\n","print(no_of_countries)"]},{"cell_type":"code","execution_count":158,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['united states' 'britain' 'united kingdom' 'u.k' 'sa' 'america' nan 's.a'\n"," 'england' 'uk' '' 'united states of america' 's. africasouth africa']\n","12\n"]}],"source":["df[\"country\"] = df[\"country\"].str.strip(\"/\")\n","df[\"country\"] = df[\"country\"].str.strip(\".\")\n","countries = df[\"country\"].unique()\n","no_of_countries = df[\"country\"].nunique()\n","print(countries)\n","print(no_of_countries)"]},{"cell_type":"code","execution_count":159,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['united states' 'united kingdom' 'south africa' nan '']\n"]}],"source":["df.replace(\"britain\",\"united kingdom\",inplace=True)\n","df.replace(\"uk\",\"united kingdom\",inplace=True)\n","df.replace(\"u.k\",\"united kingdom\",inplace=True)\n","df.replace(\"england\",\"united kingdom\",inplace=True)\n","df.replace(\"america\",\"united states\",inplace=True)\n","df.replace(\"united states of america\",\"united states\",inplace=True)\n","df.replace(\"sa\",\"south africa\",inplace=True)\n","df.replace(\"s.a\",\"south africa\",inplace=True)\n","df.replace(\"s. africasouth africa\",\"south africa\",inplace=True)\n","\n","countries = df[\"country\"].unique()\n","print(countries)"]},{"cell_type":"markdown","metadata":{"id":"UJZDMTwP16Ws"},"source":["3. Create a new column called `days_ago` in the DataFrame that is a copy of the 'date_measured' column but instead it is a number that shows how many days ago it was measured from the current date. Note that the current date can be obtained using `datetime.date.today()`."]},{"cell_type":"code","execution_count":167,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>store_name</th>\n","      <th>store_email</th>\n","      <th>department</th>\n","      <th>income</th>\n","      <th>date_measured</th>\n","      <th>country</th>\n","      <th>date_parsed</th>\n","      <th>days_ago</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>Cullen/Frost Bankers, Inc.</td>\n","      <td>NaN</td>\n","      <td>Clothing</td>\n","      <td>$54438554.24</td>\n","      <td>4-2-2006</td>\n","      <td>united states</td>\n","      <td>2006-02-04</td>\n","      <td>6656 days, 0:00:00</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>Nordson Corporation</td>\n","      <td>NaN</td>\n","      <td>Tools</td>\n","      <td>$41744177.01</td>\n","      <td>4-1-2006</td>\n","      <td>united kingdom</td>\n","      <td>2006-01-04</td>\n","      <td>6687 days, 0:00:00</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>Stag Industrial, Inc.</td>\n","      <td>NaN</td>\n","      <td>Beauty</td>\n","      <td>$36152340.34</td>\n","      <td>12-9-2003</td>\n","      <td>united states</td>\n","      <td>2003-09-12</td>\n","      <td>7532 days, 0:00:00</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>FIRST REPUBLIC BANK</td>\n","      <td>ecanadine3@fc2.com</td>\n","      <td>Automotive</td>\n","      <td>$8928350.04</td>\n","      <td>8-5-2006</td>\n","      <td>united kingdom</td>\n","      <td>2006-05-08</td>\n","      <td>6563 days, 0:00:00</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>Mercantile Bank Corporation</td>\n","      <td>NaN</td>\n","      <td>Baby</td>\n","      <td>$33552742.32</td>\n","      <td>21-1-1973</td>\n","      <td>united kingdom</td>\n","      <td>1973-01-21</td>\n","      <td>18723 days, 0:00:00</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id                   store_name         store_email  department  \\\n","0   1   Cullen/Frost Bankers, Inc.                 NaN    Clothing   \n","1   2          Nordson Corporation                 NaN       Tools   \n","2   3        Stag Industrial, Inc.                 NaN      Beauty   \n","3   4          FIRST REPUBLIC BANK  ecanadine3@fc2.com  Automotive   \n","4   5  Mercantile Bank Corporation                 NaN        Baby   \n","\n","         income date_measured         country date_parsed             days_ago  \n","0  $54438554.24      4-2-2006   united states  2006-02-04   6656 days, 0:00:00  \n","1  $41744177.01      4-1-2006  united kingdom  2006-01-04   6687 days, 0:00:00  \n","2  $36152340.34     12-9-2003   united states  2003-09-12   7532 days, 0:00:00  \n","3   $8928350.04      8-5-2006  united kingdom  2006-05-08   6563 days, 0:00:00  \n","4  $33552742.32     21-1-1973  united kingdom  1973-01-21  18723 days, 0:00:00  "]},"execution_count":167,"metadata":{},"output_type":"execute_result"}],"source":["from datetime import date\n","df[\"date_parsed\"] = pd.to_datetime(df[\"date_measured\"], format=\"%d-%m-%Y\")\n","df[\"days_ago\"]= datetime.date.today() - df[\"date_parsed\"].dt.date\n","df.head()"]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3.10.0 ('phd')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"},"vscode":{"interpreter":{"hash":"63d17dc58a06b6a6d4136fb13c245dafcf53668da37b1c3052c24d689135f5bb"}}},"nbformat":4,"nbformat_minor":0}
